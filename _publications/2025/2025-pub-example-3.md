---
title:          "Self-Rewarding Vision-Language Model via Reasoning Decomposition"
date:           2025-08-27 00:01:00 +0800
selected:       true
pub:            "Preprint"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2025"
# semantic_scholar_id: 204e3073870fae3d05bcbc2f6a8e263d9b72e776  # use this to retrieve citation count
abstract: >-
  Vision-SR1 trains VLMs to “look first, then reason” by splitting reasoning into (1) a self-generated visual perception of the image and (2) language reasoning that answers the question. Using the model’s ability to answer from its own perception as a self-reward lets it improve visual grounding without human labels or external rewards, improving visual reasoning abilities.
cover:          /assets/images/covers/vsr1-method.png
authors:
  - Zongxia Li
  - Wenhao Yu
  - Chengsong Huang
  - Rui Liu
  - Zhenwen Liang
  - Fuxiao Liu
  - Jingxi Che
  - Dian Yu
  - Jordan Boyd-Graber
  - Haitao Mi
  - Dong Yu

links:
  Code: https://github.com/zli12321/Vision-SR1
  Paper: https://arxiv.org/abs/2508.19652
---
