---
title:          "R-Zero: Self-Evolving Reasoning LLM from Zero Data"
date:           2025-08-07 00:01:00 +0800
selected:       true
pub:            "Preprint"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2025"
# semantic_scholar_id: 204e3073870fae3d05bcbc2f6a8e263d9b72e776  # use this to retrieve citation count
abstract: >-
  R-Zero trains large language models entirely without human-curated data by pitting two copies of the base model against each other: a Challenger that invents tasks just beyond the Solver’s reach and a Solver that learns to solve them. This self-evolving curriculum steadily pushes the model’s reasoning skills higher, boosting a 4B Qwen3 model by 6–8 points on math and general reasoning benchmarks.
cover:          /assets/images/covers/R0-method.png
authors:
  - Chengsong Huang
  - Wenhao Yu
  - Xiaoyang Wang
  - Hongming Zhang
  - Zongxia Li
  - Ruosen Li
  - Jiaxin Huang
  - Haitao Mi
  - Dong Yu

links:
  Code: https://github.com/Chengsong-Huang/R-Zero
  Paper: https://arxiv.org/abs/2508.05004
---
